# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i3V6NTaEcu0aC1c9iJagiRtydPcY61QC

**Improting libraries**
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import RandomOverSampler

"""Preprocessign (**Cleaning** the data for better accuracy)"""

# Load Data
df = pd.read_excel('D:/AI Model/Project2.xlsx')

# Convert percentage values to decimals
#The model can't handle symbols like % so I removed it
percent_cols = ['صافي الربح', 'العائد على الأصول ROA', 'العائد على حقوق الملكية ROI']
for col in percent_cols:
    df[col] = df[col].astype(str).str.rstrip('%').astype(float) / 100  # تحويل 13% → 0.13

df.head()

# Fill missing values with median
df.fillna(df.median(numeric_only=True), inplace=True)

# Encode target variable (that we need to expect later)
#Because AI models can manage numeric values more than categorical so I encoded it like any unique value take a numeric value (A->0 and so on)
label_encoder = LabelEncoder()
df['التصنيف_رقمي'] = label_encoder.fit_transform(df['التصنيف'])
y = df['التصنيف_رقمي']

dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))

# Normalize numeric features using Min-Max Scaling (range: 0 to 1)
# This ensures all features contribute equally to the model, preventing bias from large values.
# Particularly useful for models like KNN, SVM, and Neural Networks.
numeric_cols = df.select_dtypes(include=[np.number]).columns.drop(['التصنيف_رقمي'])
scaler = StandardScaler()
X = scaler.fit_transform(df[numeric_cols])

# Feature selection
# This improves model performance by removing less informative features.
selector = SelectKBest(score_func=f_classif, k=8)  # Select top 8 features
X_selected = selector.fit_transform(X, y)

print("Class distribution in dataset:\n", np.unique(y, return_counts=True))

# Apply Random Over-Sampling to balance classes
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_selected, y)

unique_classes, class_counts = np.unique(y_resampled, return_counts=True)
print("Balanced Class Distribution:", dict(zip(unique_classes, class_counts)))

from sklearn.model_selection import train_test_split

# Split the balanced dataset
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled
)

# Train the XGBoost Model
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

model = XGBClassifier(
    objective='multi:softmax',
    num_classes=len(np.unique(y_train)),
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    eval_metric='merror'
)

model.fit(X_train, y_train)

# Make Predictions
y_pred = model.predict(X_test)

# Evaluate Model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

# Feature selection
selector = SelectKBest(score_func=f_classif, k=8)  # Select top 8 features
X_selected = selector.fit_transform(X, y)

# Get the names of the selected features
selected_features = np.array(numeric_cols)[selector.get_support()]
print("Selected Features:", selected_features)

import joblib

# Save the trained model, selector, scaler, and label encoder
joblib.dump(model, 'xgb_model.pkl')
joblib.dump(selector, 'selector.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')

print("Model and preprocessing objects have been saved.")


